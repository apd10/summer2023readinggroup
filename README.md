# Summer 2023 Reading Group

## Goals. 
  1. create a strong background required for research in ML Efficiency
  2. Get aquanited with promising approaches for ML Efficiency
  3. Formulate new ideas to work on in Fall'23

## List of readings

|        **Paper**        | **Read by** | **Presenter** | **Presentation date** |
|:-----------------------:|:-----------:|:-------------:|:---------------------:|
| [High Dimensional Spaces](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf) ( [slides](https://docs.google.com/presentation/d/1SR3UXdEe5lOt92YrFixN9HNsBYloNsoZigLBoe_zlCc/edit?usp=sharing) ) |     All     |  Aditya Desai |          06/13/2023          |
|||||
| [Survey on Quantization methods](https://arxiv.org/pdf/2103.13630.pdf) ( [slides](https://docs.google.com/presentation/d/1P1saT0cNrDkpbGwzQKQZDF6jU9h0uhEK92P2wcMO2HM/edit?usp=sharing)) | Kevin, Aditya       | Kevin              |     06/20/2023  |
| [Transformer full stack optimization](https://arxiv.org/pdf/2302.14017.pdf) ( [slides](https://docs.google.com/presentation/d/1QYrTFgImQodIdMBzH_l5tIiyijYf68lXlVD6T-j-76M/edit?usp=sharing) ) | Ben, Gaurav             | Ben              |   06/20/2023 |
| [Survey on Model compression methods](https://ieeexplore.ieee.org/abstract/document/9043731) ( [slides](https://docs.google.com/presentation/d/1Z5BIwrF0vlWZKSt0iDdcYbSfr_ySxJ6aGqavJGUoWbM/edit?usp=sharing))| Apoorv             | Apoorv              |   06/20/2023 |
|||||
| LTH and follow ups ([D1](https://arxiv.org/abs/1803.03635), [D2](https://arxiv.org/pdf/1903.01611.pdf), [D3](https://arxiv.org/pdf/2009.08576.pdf))| Aditya | Aditya              |   06/27/2023 |
| [NLP+retrival : Prompt survey](https://dl.acm.org/doi/pdf/10.1145/3560815)                        |  Gaurav           |   Gaurav            | 06/27/2023   |
| Distributed mean estimation and applications | Ben | Ben              |   07/04/2023 |
| Efficient attention                        |  Kevin           |   Kevin            | 07/04/2023   |


## Themes (Ignore the order of papers)

## A. Sparsity based efficiency in ML ( Dr. Chris Re's body of work )
1. 	Monarch: Expressive Structured Matrices for Efficient and Accurate Training
2. Pixelated Butterfly: Simple and Efficient Sparses Training for Neural Network Models 
3. Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time
4. Scatterbrain: Unifying Sparse and Low-rank Attention
5. Mongoose: A Learnable LSH Framework for Efficient Neural Network Training.

## B. Efficient Attention ( Dr. Chris Re's body of work )
1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 
2. Efficiently Modeling Long Sequences with Structured State Spaces
3. Simple Hardware-Efficient Long Convolutions for Sequence Modeling.

## C. Distributed ML and randomized solutions (Dr. Mitzenmachers's body of work)
1. THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression
2. Eden: Communication-efficient and robust distributed mean estimation for federated learning
3. QUICK-FL: Quick Unbiased Compression for Federated Learning
4. Drive: One-bit distributed mean estimation

## D. Deep learning understanding via pruning. (Dr. Carbin's body of work )
1. <s>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks </s>
2. <s>Stabilizing the Lottery Ticket Hypothesis / The Lottery Ticket Hypothesis at Scale </s>
3. <s> The Early Phase of Neural Network Training </s>
4. Pruning Neural Networks at Initialization: Why are We Missing the Mark?
5. The Effect of Data Dimensionality on Neural Network Prunability






### Other interesting links

1. [Interactive linear algebra](https://textbooks.math.gatech.edu/ila/)
2. [Plot 3D graphs online](https://www.geogebra.org/)
3. [Plot 2D graphs online](https://www.desmos.com/calculator)
